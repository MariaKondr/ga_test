{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Suitability Analysis\n",
    "\n",
    "This notebook demonstrates the workflow for finding optimal pipeline paths using geospatial suitability analysis. The analysis consists of:\n",
    "\n",
    "1. Loading raster factors and reference paths\n",
    "2. Creating cost maps with weighted factors\n",
    "3. Finding least cost paths between points\n",
    "4. Comparing predicted paths with reference paths\n",
    "5. Optimizing weights to improve path prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom pipeline analysis modules\n",
    "from pipefit.data_loader import DataLoader, load_reference_path\n",
    "from pipefit.cost_map import CostMapGenerator\n",
    "from pipefit.path_finder import LeastCostPathFinder\n",
    "from pipefit.path_evaluator import PathEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "First, we'll load a dataset containing raster factors and reference path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to your dataset folder\n",
    "dataset_path = \"path/to/your/dataset\"\n",
    "\n",
    "# Define a factor mapping (optional)\n",
    "factor_mapping = {\n",
    "    'elev.tif': 'elevation',        # Map filename to standardized name\n",
    "    'protected.tif': 'protected_areas',\n",
    "    'cities.tif': 'proximity_to_cities',\n",
    "    'slope.tif': 'slope',\n",
    "    'soil.tif': 'soil_type'\n",
    "}\n",
    "\n",
    "# Create a data loader and load the data\n",
    "data_loader = DataLoader(dataset_path)\n",
    "data_loader.load_data(factor_mapping=factor_mapping)\n",
    "\n",
    "# Load reference path separately\n",
    "reference_path_dir = os.path.join(dataset_path, 'reference')\n",
    "reference_path = load_reference_path(reference_path_dir)\n",
    "\n",
    "# Check loaded factors\n",
    "print(f\"Loaded factors: {list(data_loader.factors.keys())}\")\n",
    "print(f\"Reference path loaded: {reference_path is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Input Factors\n",
    "\n",
    "Let's examine the normalized raster factors that we'll use to create the cost map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cost map generator\n",
    "cost_generator = CostMapGenerator(data_loader.factors)\n",
    "\n",
    "# Visualize all input factors\n",
    "cost_generator.visualize_factors(figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Cost Map\n",
    "\n",
    "Now we'll create a cost map by combining factors with weights. The weights reflect the relative importance of each factor in determining pipeline suitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights for each factor\n",
    "# These can be different from the factor names in the data files\n",
    "weights = {\n",
    "    'elevation': 0.3,        # Higher weight for elevation\n",
    "    'protected_areas': 0.2,  # Medium weight for protected areas\n",
    "    'proximity_to_cities': 0.2,  # Medium weight for city proximity\n",
    "    'slope': 0.15,           # Lower weight for slope\n",
    "    'soil_type': 0.15        # Lower weight for soil type\n",
    "}\n",
    "\n",
    "# Optional: Define a weight-to-factor mapping if your weight keys don't match factor names\n",
    "weight_mapping = None  # We're using standardized names, so no mapping needed\n",
    "\n",
    "# Generate cost map with default weight for any missing factors\n",
    "cost_map = cost_generator.generate_cost_map(\n",
    "    weights=weights,\n",
    "    default_weight=0.05,  # Any factors without explicit weights get this value\n",
    "    weight_mapping=weight_mapping\n",
    ")\n",
    "\n",
    "# Visualize cost map\n",
    "cost_generator.visualize_cost_map(cost_map, title=\"Weighted Cost Map\")\n",
    "\n",
    "# Save cost map (optional)\n",
    "cost_map_path = os.path.join(dataset_path, 'results', 'cost_map.tif')\n",
    "os.makedirs(os.path.dirname(cost_map_path), exist_ok=True)\n",
    "data_loader.save_raster(cost_map, cost_map_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find Least Cost Path\n",
    "\n",
    "Using the cost map, we'll find the least cost path between start and end points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start and end points\n",
    "# Option 1: Define using geographical coordinates\n",
    "start_x, start_y = -120.5, 38.2  # Example coordinates (replace with actual coordinates)\n",
    "end_x, end_y = -119.8, 38.7     # Example coordinates (replace with actual coordinates)\n",
    "\n",
    "# Convert to raster indices\n",
    "start_indices = data_loader.coordinates_to_indices(start_x, start_y)\n",
    "end_indices = data_loader.coordinates_to_indices(end_x, end_y)\n",
    "\n",
    "# Option 2: Define directly as raster indices\n",
    "# start_indices = (100, 50)   # Example indices (row, col) (replace with actual indices)\n",
    "# end_indices = (400, 300)    # Example indices (row, col) (replace with actual indices)\n",
    "\n",
    "# Option 3: Use default start/end points (lower left to upper right)\n",
    "# start_indices = None  # Will use lower left corner\n",
    "# end_indices = None    # Will use upper right corner\n",
    "\n",
    "print(f\"Start point: {start_indices}, End point: {end_indices}\")\n",
    "\n",
    "# Create path finder (no need to pass transform/crs at initialization)\n",
    "path_finder = LeastCostPathFinder(cost_map)\n",
    "\n",
    "# Get metadata for later use\n",
    "metadata = data_loader.get_common_metadata()\n",
    "\n",
    "# Find least cost path using A* algorithm (default)\n",
    "path_indices, path_cost = path_finder.find_path(\n",
    "    start_indices, \n",
    "    end_indices, \n",
    "    method='astar',  # 'astar' or 'dijkstra'\n",
    "    fully_connected=True  # Use 8-connected neighbors\n",
    ")\n",
    "print(f\"Path found with total cost: {path_cost}\")\n",
    "\n",
    "# Visualize path on cost map\n",
    "path_finder.visualize_path(path_indices, start_indices, end_indices)\n",
    "\n",
    "# Convert path to GeoDataFrame (pass transform and crs here)\n",
    "path_gdf = path_finder.path_to_geodataframe(\n",
    "    path_indices, \n",
    "    path_cost, \n",
    "    transform=metadata['transform'], \n",
    "    crs=metadata['crs']\n",
    ")\n",
    "\n",
    "# Save path (optional)\n",
    "path_output = os.path.join(dataset_path, 'results', 'least_cost_path.shp')\n",
    "path_gdf.to_file(path_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare with Reference Path\n",
    "\n",
    "Now we'll compare our predicted path with the reference (ground truth) path and calculate similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path evaluator with reference path passed explicitly\n",
    "evaluator = PathEvaluator(\n",
    "    predicted_path=path_indices,\n",
    "    reference_path=reference_path,  # Pass the reference path directly\n",
    "    transform=metadata['transform'],\n",
    "    crs=metadata['crs']\n",
    ")\n",
    "\n",
    "# Calculate all metrics\n",
    "metrics = evaluator.evaluate_all_metrics(buffer_distance=100)  # Adjust buffer distance as needed\n",
    "\n",
    "# Display metrics\n",
    "print(\"Path Comparison Metrics:\")\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "# Visualize paths with metrics\n",
    "evaluator.visualize_paths(title=\"Predicted vs Reference Path\", buffer_distance=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment with Different Weights\n",
    "\n",
    "Let's experiment with different weight combinations to see which produces paths that better match the reference path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define several weight combinations to try\n",
    "weight_combinations = [\n",
    "    {\n",
    "        'elevation': 0.5,\n",
    "        'protected_areas': 0.2,\n",
    "        'proximity_to_cities': 0.1,\n",
    "        'slope': 0.1,\n",
    "        'soil_type': 0.1\n",
    "    },\n",
    "    {\n",
    "        'elevation': 0.2,\n",
    "        'protected_areas': 0.4,\n",
    "        'proximity_to_cities': 0.2,\n",
    "        'slope': 0.1,\n",
    "        'soil_type': 0.1\n",
    "    },\n",
    "    {\n",
    "        'elevation': 0.2,\n",
    "        'protected_areas': 0.2,\n",
    "        'proximity_to_cities': 0.4,\n",
    "        'slope': 0.1,\n",
    "        'soil_type': 0.1\n",
    "    }\n",
    "]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Try each weight combination\n",
    "for i, weights in enumerate(weight_combinations):\n",
    "    print(f\"\\nTrying weight combination {i+1}:\")\n",
    "    print(weights)\n",
    "    \n",
    "    # Generate cost map\n",
    "    cost_map = cost_generator.generate_cost_map(\n",
    "        weights=weights,\n",
    "        default_weight=0.05  # Use a default weight for any missing factors\n",
    "    )\n",
    "    \n",
    "    # Create path finder\n",
    "    path_finder = LeastCostPathFinder(cost_map)\n",
    "    \n",
    "    # Find least cost path using A* algorithm\n",
    "    path_indices, path_cost = path_finder.find_path(\n",
    "        start_indices, \n",
    "        end_indices, \n",
    "        method='astar'\n",
    "    )\n",
    "    \n",
    "    # Evaluate path\n",
    "    evaluator = PathEvaluator(\n",
    "        predicted_path=path_indices,\n",
    "        reference_path=reference_path,  # Pass reference path directly\n",
    "        transform=metadata['transform'],\n",
    "        crs=metadata['crs']\n",
    "    )\n",
    "    \n",
    "    metrics = evaluator.evaluate_all_metrics(buffer_distance=100)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'weights': weights,\n",
    "        'path_cost': path_cost,\n",
    "        'metrics': metrics\n",
    "    })\n",
    "    \n",
    "    # Display metrics\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    evaluator.visualize_paths(title=f\"Path Comparison for Weight Combination {i+1}\", \n",
    "                             buffer_distance=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Identify Best Weight Combination\n",
    "\n",
    "Now we'll identify which weight combination produced the best match to the reference path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best combination based on the combined score\n",
    "combined_scores = [result['metrics']['combined_score'] for result in results]\n",
    "best_index = combined_scores.index(min(combined_scores))\n",
    "best_weights = results[best_index]['weights']\n",
    "\n",
    "print(\"\\nBest weight combination:\")\n",
    "for factor, weight in best_weights.items():\n",
    "    print(f\"{factor}: {weight:.2f}\")\n",
    "\n",
    "print(\"\\nMetrics for best combination:\")\n",
    "for metric_name, value in results[best_index]['metrics'].items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Results\n",
    "\n",
    "Finally, let's save the best cost map and path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final cost map with best weights\n",
    "final_cost_map = cost_generator.generate_cost_map(\n",
    "    weights=best_weights,\n",
    "    default_weight=0.05\n",
    ")\n",
    "\n",
    "# Find final path using A* algorithm\n",
    "path_finder = LeastCostPathFinder(final_cost_map)\n",
    "final_path_indices, final_path_cost = path_finder.find_path(\n",
    "    start_indices, \n",
    "    end_indices, \n",
    "    method='astar'\n",
    ")\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "final_path_gdf = path_finder.path_to_geodataframe(\n",
    "    final_path_indices, \n",
    "    final_path_cost, \n",
    "    transform=metadata['transform'], \n",
    "    crs=metadata['crs']\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results_dir = os.path.join(dataset_path, 'results')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save cost map\n",
    "final_cost_map_path = os.path.join(results_dir, 'final_cost_map.tif')\n",
    "data_loader.save_raster(final_cost_map, final_cost_map_path)\n",
    "\n",
    "# Save path\n",
    "final_path_output = os.path.join(results_dir, 'final_least_cost_path.shp')\n",
    "final_path_gdf.to_file(final_path_output)\n",
    "\n",
    "print(f\"Saved final cost map to: {final_cost_map_path}\")\n",
    "print(f\"Saved final path to: {final_path_output}\")\n",
    "\n",
    "# Final visualization\n",
    "evaluator = PathEvaluator(\n",
    "    predicted_path=final_path_indices,\n",
    "    reference_path=reference_path,  # Pass reference path directly\n",
    "    transform=metadata['transform'],\n",
    "    crs=metadata['crs']\n",
    ")\n",
    "evaluator.visualize_paths(title=\"Final Optimized Path vs Reference Path\", buffer_distance=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Automated Weight Optimization\n",
    "\n",
    "Let's use our optimization tools to automatically find the best weights for the factors.\n",
    "\n",
    "### 9.1 Simple Gradient Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the optimizer\n",
    "from pipeline.weight_optimizer import SimpleGradientOptimizer\n",
    "\n",
    "# Define the factor names to optimize\n",
    "factor_names = list(data_loader.factors.keys())\n",
    "print(f\"Factors to optimize: {factor_names}\")\n",
    "\n",
    "# Set up datasets for optimization\n",
    "# For this example, we'll use a single dataset, but you can add more\n",
    "dataset_paths = [dataset_path]\n",
    "\n",
    "# Initial weights (starting point for optimization)\n",
    "initial_weights = {factor: 1.0/len(factor_names) for factor in factor_names}\n",
    "print(f\"Initial weights: {initial_weights}\")\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = SimpleGradientOptimizer(\n",
    "    datasets=dataset_paths,\n",
    "    factors=factor_names,\n",
    "    initial_weights=initial_weights,\n",
    "    learning_rate=0.05,\n",
    "    regularization=0.01\n",
    ")\n",
    "\n",
    "# Run the optimization process\n",
    "best_weights, scores_history = optimizer.optimize(\n",
    "    num_iterations=50,\n",
    "    early_stopping_patience=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Plot the optimization history\n",
    "optimizer.plot_optimization_history(scores_history)\n",
    "\n",
    "# Save the optimized weights\n",
    "weights_output = os.path.join(dataset_path, 'results', 'optimized_weights.json')\n",
    "optimizer.save_weights(weights_output)\n",
    "\n",
    "print(\"\\nOptimized weights:\")\n",
    "for factor, weight in best_weights.items():\n",
    "    print(f\"{factor}: {weight:.4f}\")\n",
    "\n",
    "# Generate final cost map with optimized weights\n",
    "final_cost_map = cost_generator.generate_cost_map(best_weights)\n",
    "\n",
    "# Create a new path with optimized weights\n",
    "path_finder = LeastCostPathFinder(final_cost_map)\n",
    "optimized_path_indices, optimized_path_cost = path_finder.find_path(start_indices, end_indices)\n",
    "\n",
    "# Evaluate the optimized path\n",
    "optimized_evaluator = PathEvaluator(\n",
    "    predicted_path=optimized_path_indices,\n",
    "    reference_path=reference_path,\n",
    "    transform=metadata['transform'],\n",
    "    crs=metadata['crs']\n",
    ")\n",
    "\n",
    "optimized_metrics = optimized_evaluator.evaluate_all_metrics()\n",
    "\n",
    "print(\"\\nOptimized Path Metrics:\")\n",
    "for metric_name, value in optimized_metrics.items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "# Visualize the optimized path\n",
    "optimized_evaluator.visualize_paths(\n",
    "    title=\"Optimized Path vs Reference Path\",\n",
    "    buffer_distance=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Comparing Different Path-Finding Algorithms\n",
    "\n",
    "Let's compare the A* algorithm with Dijkstra's algorithm using our optimized weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare A* and Dijkstra algorithms with optimized weights\n",
    "print(\"\\nComparing path-finding algorithms with optimized weights...\")\n",
    "\n",
    "# Generate cost map with optimized weights\n",
    "comparison_cost_map = cost_generator.generate_cost_map(\n",
    "    weights=best_weights,\n",
    "    default_weight=0.05\n",
    ")\n",
    "\n",
    "# Create path finder\n",
    "path_finder = LeastCostPathFinder(comparison_cost_map)\n",
    "\n",
    "# Find paths using both algorithms\n",
    "astar_start_time = time.time()\n",
    "astar_path_indices, astar_path_cost = path_finder.find_path(\n",
    "    start_indices,\n",
    "    end_indices,\n",
    "    method='astar'\n",
    ")\n",
    "astar_time = time.time() - astar_start_time\n",
    "\n",
    "dijkstra_start_time = time.time()\n",
    "dijkstra_path_indices, dijkstra_path_cost = path_finder.find_path(\n",
    "    start_indices,\n",
    "    end_indices,\n",
    "    method='dijkstra'\n",
    ")\n",
    "dijkstra_time = time.time() - dijkstra_start_time\n",
    "\n",
    "# Display results\n",
    "print(f\"A* algorithm results:\")\n",
    "print(f\"  Path cost: {astar_path_cost:.4f}\")\n",
    "print(f\"  Execution time: {astar_time:.4f} seconds\")\n",
    "print(f\"  Path length: {len(astar_path_indices)} points\")\n",
    "\n",
    "print(f\"\\nDijkstra's algorithm results:\")\n",
    "print(f\"  Path cost: {dijkstra_path_cost:.4f}\")\n",
    "print(f\"  Execution time: {dijkstra_time:.4f} seconds\")\n",
    "print(f\"  Path length: {len(dijkstra_path_indices)} points\")\n",
    "\n",
    "# Evaluate both paths\n",
    "astar_evaluator = PathEvaluator(\n",
    "    predicted_path=astar_path_indices,\n",
    "    reference_path=reference_path,\n",
    "    transform=metadata['transform'],\n",
    "    crs=metadata['crs']\n",
    ")\n",
    "\n",
    "dijkstra_evaluator = PathEvaluator(\n",
    "    predicted_path=dijkstra_path_indices,\n",
    "    reference_path=reference_path,\n",
    "    transform=metadata['transform'],\n",
    "    crs=metadata['crs']\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "astar_metrics = astar_evaluator.evaluate_all_metrics()\n",
    "dijkstra_metrics = dijkstra_evaluator.evaluate_all_metrics()\n",
    "\n",
    "print(\"\\nA* metrics:\")\n",
    "for metric_name, value in astar_metrics.items():\n",
    "    print(f\"  {metric_name}: {value:.4f}\")\n",
    "    \n",
    "print(\"\\nDijkstra metrics:\")\n",
    "for metric_name, value in dijkstra_metrics.items():\n",
    "    print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "# Visualize both paths together\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(comparison_cost_map, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(label='Cost')\n",
    "\n",
    "# Plot the paths\n",
    "astar_rows, astar_cols = astar_path_indices[:, 0], astar_path_indices[:, 1]\n",
    "dijkstra_rows, dijkstra_cols = dijkstra_path_indices[:, 0], dijkstra_path_indices[:, 1]\n",
    "\n",
    "plt.plot(astar_cols, astar_rows, 'r-', linewidth=2, label='A* Path')\n",
    "plt.plot(dijkstra_cols, dijkstra_rows, 'b--', linewidth=2, label='Dijkstra Path')\n",
    "plt.plot(start_indices[1], start_indices[0], 'go', markersize=10, label='Start')\n",
    "plt.plot(end_indices[1], end_indices[0], 'mo', markersize=10, label='End')\n",
    "\n",
    "plt.title('Comparison of Path-Finding Algorithms')\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Neural Network-Based Weight Prediction (Extension)\n",
    "\n",
    "Now let's explore how to use a neural network to predict weights based on dataset features:\n",
    "\n",
    "```python\n",
    "from pipeline.weight_optimizer import NeuralWeightPredictor, extract_dataset_features\n",
    "import numpy as np\n",
    "\n",
    "# For demonstration purposes, let's assume we have multiple datasets\n",
    "# and we've already found optimal weights for each\n",
    "\n",
    "# Create example dataset for training\n",
    "num_datasets = 5\n",
    "feature_dim = 20  # Number of features per dataset\n",
    "\n",
    "# Create dummy training data (in a real scenario, this would be extracted from real datasets)\n",
    "X_train = np.random.rand(num_datasets, feature_dim)\n",
    "y_train = np.random.rand(num_datasets, len(factor_names))\n",
    "y_train = y_train / y_train.sum(axis=1, keepdims=True)  # Normalize weights to sum to 1\n",
    "\n",
    "# Create and train the neural weight predictor\n",
    "nn_predictor = NeuralWeightPredictor(input_shape=(feature_dim,))\n",
    "nn_predictor.factor_names = factor_names  # Set factor names for the model\n",
    "\n",
    "# Train the model\n",
    "history = nn_predictor.train(X_train, y_train, epochs=30, batch_size=2)\n",
    "\n",
    "# Now we can use the model to predict weights for a new dataset\n",
    "# Extract features from our dataset\n",
    "dataset_features = extract_dataset_features(data_loader)\n",
    "\n",
    "# Predict weights\n",
    "predicted_weights = nn_predictor.predict_weights(dataset_features)\n",
    "\n",
    "print(\"\\nNeural Network Predicted Weights:\")\n",
    "for factor, weight in predicted_weights.items():\n",
    "    print(f\"{factor}: {weight:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Process Multiple Datasets\n",
    "\n",
    "Let's use the enhanced `process_dataset` function to handle multiple datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_path, weights, start_coords=None, end_coords=None, factor_mapping=None):\n",
    "    \"\"\"Process a single dataset and return metrics.\"\"\"\n",
    "    # Load data\n",
    "    data_loader = DataLoader(dataset_path)\n",
    "    data_loader.load_data(factor_mapping=factor_mapping)\n",
    "    \n",
    "    # Load reference path\n",
    "    reference_path_dir = os.path.join(dataset_path, 'reference')\n",
    "    reference_path = load_reference_path(reference_path_dir)\n",
    "    \n",
    "    # Generate cost map\n",
    "    cost_generator = CostMapGenerator(data_loader.factors)\n",
    "    cost_map = cost_generator.generate_cost_map(\n",
    "        weights=weights,\n",
    "        default_weight=0.05\n",
    "    )\n",
    "    \n",
    "    # Get metadata\n",
    "    metadata = data_loader.get_common_metadata()\n",
    "    \n",
    "    # Determine start and end points\n",
    "    if start_coords and end_coords:\n",
    "        start_indices = data_loader.coordinates_to_indices(start_coords[0], start_coords[1])\n",
    "        end_indices = data_loader.coordinates_to_indices(end_coords[0], end_coords[1])\n",
    "    else:\n",
    "        # Use first and last points of reference path as default\n",
    "        ref_geom = reference_path.geometry.iloc[0]\n",
    "        start_coords = (ref_geom.coords[0][0], ref_geom.coords[0][1])\n",
    "        end_coords = (ref_geom.coords[-1][0], ref_geom.coords[-1][1])\n",
    "        start_indices = data_loader.coordinates_to_indices(start_coords[0], start_coords[1])\n",
    "        end_indices = data_loader.coordinates_to_indices(end_coords[0], end_coords[1])\n",
    "    \n",
    "    # Find path using A* algorithm\n",
    "    path_finder = LeastCostPathFinder(cost_map)\n",
    "    path_indices, path_cost = path_finder.find_path(\n",
    "        start_indices, \n",
    "        end_indices, \n",
    "        method='astar'\n",
    "    )\n",
    "    \n",
    "    # Evaluate path\n",
    "    evaluator = PathEvaluator(\n",
    "        predicted_path=path_indices,\n",
    "        reference_path=reference_path,  # Pass reference path directly\n",
    "        transform=metadata['transform'],\n",
    "        crs=metadata['crs']\n",
    "    )\n",
    "    \n",
    "    metrics = evaluator.evaluate_all_metrics()\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'dataset': os.path.basename(dataset_path),\n",
    "        'cost_map': cost_map,\n",
    "        'path_indices': path_indices,\n",
    "        'path_cost': path_cost,\n",
    "        'metrics': metrics,\n",
    "        'data_loader': data_loader,\n",
    "        'metadata': metadata,\n",
    "        'reference_path': reference_path\n",
    "    }\n",
    "\n",
    "# Define factor and weight mappings (if needed)\n",
    "factor_mappings = {\n",
    "    'dataset1': {\n",
    "        'elev.tif': 'elevation',\n",
    "        'protected.tif': 'protected_areas',\n",
    "        'cities.tif': 'proximity_to_cities',\n",
    "        'slope.tif': 'slope',\n",
    "        'soil.tif': 'soil_type'\n",
    "    },\n",
    "    'dataset2': {\n",
    "        'dem.tif': 'elevation',\n",
    "        'parks.tif': 'protected_areas',\n",
    "        'urban.tif': 'proximity_to_cities'\n",
    "        # Missing factors will use default weight\n",
    "    }\n",
    "}\n",
    "\n",
    "# Process all datasets with the optimized weights\n",
    "all_results = []\n",
    "for dataset_path in dataset_paths:\n",
    "    dataset_name = os.path.basename(dataset_path)\n",
    "    dataset_factor_mapping = factor_mappings.get(dataset_name)\n",
    "    \n",
    "    result = process_dataset(\n",
    "        dataset_path, \n",
    "        best_weights,\n",
    "        factor_mapping=dataset_factor_mapping\n",
    "    )\n",
    "    all_results.append(result)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nDataset: {result['dataset']}\")\n",
    "    for metric_name, value in result['metrics'].items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the enhanced pipeline suitability analysis framework:\n",
    "\n",
    "1. We loaded raster factors with support for custom factor name mapping\n",
    "2. We loaded reference paths separately from the data loader\n",
    "3. We created cost maps with weighted factors, supporting default weights and factor mapping\n",
    "4. We found least cost paths using two algorithms: A* (more efficient) and Dijkstra's algorithm\n",
    "5. We compared predicted paths with reference paths using multiple metrics\n",
    "6. We optimized weights to improve path prediction\n",
    "7. We showed how to handle datasets with different factor names and structures\n",
    "8. We compared performance and results between different path-finding algorithms\n",
    "\n",
    "The framework now provides:\n",
    "- More flexibility in handling different factor names and structures\n",
    "- Support for multiple path-finding algorithms\n",
    "- Clear separation of concerns between data loading and path evaluation\n",
    "- Default path-finding behavior for demonstration purposes\n",
    "\n",
    "This framework can be used as a foundation for further refinements, including:\n",
    "- More sophisticated neural network architectures for weight prediction\n",
    "- Adding more advanced path-finding algorithms\n",
    "- Including additional path similarity metrics\n",
    "- Scaling to larger datasets and more complex landscapes\n",
    "- Interactive visualization and interactive weight adjustment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
